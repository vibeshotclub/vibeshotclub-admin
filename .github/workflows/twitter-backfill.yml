name: Twitter Crawler Backfill

on:
  workflow_dispatch:
    inputs:
      days:
        description: '补抓最近 N 天的推文'
        required: false
        default: '10'
      max_pages:
        description: '每个用户最多翻页数'
        required: false
        default: '10'
      debug:
        description: 'Enable debug mode'
        required: false
        default: 'false'

env:
  PYTHON_VERSION: '3.11'

jobs:
  backfill:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    environment: Production

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'scripts/twitter-crawler/requirements.txt'

      - name: Install dependencies
        run: |
          cd scripts/twitter-crawler
          pip install -r requirements.txt

      - name: Run backfill
        env:
          BOT_API_KEY: ${{ secrets.BOT_API_KEY }}
          BOT_API_URL: ${{ secrets.BOT_API_URL }}
          RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
          AI_PROVIDER: ${{ vars.AI_PROVIDER }}
          CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          DEBUG: ${{ github.event.inputs.debug }}
        run: |
          cd scripts/twitter-crawler
          python backfill.py --days ${{ github.event.inputs.days }} --max-pages ${{ github.event.inputs.max_pages }}

      - name: Upload logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: backfill-logs
          path: scripts/twitter-crawler/logs/
          retention-days: 7